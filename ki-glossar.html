<!DOCTYPE html>
<html lang="de">
<head>
  <meta charset="UTF-8">
  <title>Kleines KI-Glossar</title>
  <meta name="author" content="Franz Jankowski">
  <meta name="copyright" content="© Franz Jankowski 2025. Alle Rechte vorbehalten.">
  <meta name="description" content="Glossar KI-Begriffe, Ergänzungen und Trainingsverfahren.">
  <style>
    body { font-family: Arial, sans-serif; margin: 2em; background: #fbfbfb;}
    h1, h2, h3, h4 { color: #2c3e50; }
    h1 { border-bottom: 2px solid #ccc; padding-bottom:.3em;}
    .toc { background: #eee; padding:1em; margin-bottom:2em; border-radius:8px; }
    dt { font-weight: bold; margin-top:1em;}
    dd { margin-bottom:1em; }
    .note { font-style: italic; color: #34495e;}
    .section { margin-top:2em;}
    .small { font-size:.95em; color:#888; }
    a { color:#006699; text-decoration:none; }
    a:hover { text-decoration:underline; }
  </style>
</head>
<body>
  <h1>Kleines KI-Glossar</h1>
  <div class="small">
    <span>In Unterlagen und Veranstaltungen verwendete Begriffe (v0.7d)</span>
  </div>

  <div class="toc">
    <strong>Inhalt</strong>
    <ul>
      <li><a href="#rechtshinweis">Urheberrecht & Verwendungshinweis, KI-Transparenz</a></li>
      <li><a href="#glossar">Glossar</a></li>
      <li><a href="#ergaenzungen">Ergänzungen</a></li>
      <li><a href="#feinabstimmung">Verfahren zur Feinabstimmung</a></li>
      <li><a href="#training">Verfahren zum Training von Sprachmodellen</a></li>
    </ul>
  </div>

  <section id="rechtshinweis" class="section">
    <h2>Urheberrecht & Verwendungshinweis, KI-Transparenz</h2>
    <strong>© Franz Jankowski 2025. Alle Rechte vorbehalten.</strong><br>
    Diese Unterlagen sind ausschließlich für den persönlichen, nicht-kommerziellen Gebrauch bestimmt. 
    Eine Vervielfältigung, Weitergabe oder kommerzielle Nutzung der gesamten oder von Teilen des Materials
    ist ohne schriftliche Genehmigung untersagt. Gesetzliche Schranken (Zitatrecht etc.) bleiben davon natürlich unberührt. 
    Zur <strong>KI-Transparenz</strong> siehe 
    <a href="https://franzjankowski.github.io/ki-transparenz">diese Seite</a>.
  </section>

  <section id="glossar" class="section">
    <h2>Glossar</h2>
    <dl>
      <dt>Agenten / agentisch</dt>
      <dd>Selbstständig handelnde KI-Systeme, die Aufgaben planen, ausführen und auch mit externen Diensten oder Geräten (z.B. Telefon) interagieren können, sofern Schnittstellen vorhanden sind.</dd>

      <dt>ChatBot</dt>
      <dd>Kofferwort (Wortverschmelzung) aus englisch „Chat“ (plaudern) und „Robot“ (Roboter). Textbasiertes Dialogsystem, das mithilfe von LLMs menschenähnliche Gespräche führen und komplexe Anfragen beantworten kann.</dd>

      <dt>Cloud</dt>
      <dd>Onlinedienste mit Programmen, Speicherplatz und Rechenleistung über das Internet, anstatt lokal auf dem eigenen Gerät.</dd>

      <dt>Cognitive Offloading</dt>
      <dd>Auslagern geistiger Aufgaben an Geräte, andere oder in die Zukunft – hilfreich zur Entlastung, riskant bei Gewöhnung, weil Analysefähigkeit verkümmern kann.</dd>

      <dt>Emergenz</dt>
      <dd><span class="note">Unerwartete neue Fähigkeiten großer Modelle.</span> Ab einer bestimmten Komplexität zeigen KI-Modelle plötzlich Verhaltensweisen oder Leistungen, die aus einzelnen Bausteinen oder Trainingsdaten allein nicht vorhersehbar sind.</dd>

      <dt>Filterblase</dt>
      <dd>Eingeengte Info-Welt – entsteht durch automatische Empfehlungsfunktionen oder durch bewusste Auswahl immer gleicher Kontakte und Medien. Andere Perspektiven bleiben außen vor.</dd>

      <dt>Human in the Loop</dt>
      <dd>Ansatz, bei dem der Mensch bewusst Teil eines KI-Systems bleibt – um Ergebnisse zu prüfen, Entscheidungen zu bestätigen und Verantwortung zu behalten.</dd>

      <dt>How to</dt>
      <dd>Wörtlich „Wie man ...“; Anleitung oder Schritt-für-Schritt-Beschreibung.</dd>

      <dt>Link</dt>
      <dd><strong>hier:</strong> Verknüpfung zu Internetadressen.<br><strong>allgemein:</strong> Verbindung zwischen zwei Elementen.</dd>

      <dt>LLM</dt>
      <dd>Large Language Model, Große Sprachmodelle; KI, mit der man in menschlicher Sprache kommunizieren kann.</dd>

      <dt>Lokale KI</dt>
      <dd>Läuft vollständig auf dem eigenen Rechner, Smartphone, unabhängig vom Internet. Manche Tools für lokale KIs erlauben z.B. die Einbindung von Suchmaschinen, sofern das lokale KI-Modell „Tool Use“ beherrscht und Suchen selbständig anfordern sowie die Ergebnisse entgegennehmen kann.</dd>

      <dt>Persona</dt>
      <dd><strong>Allgemein:</strong> Fiktive Figur zur Darstellung einer bestimmten Sichtweise.<br><strong>Hier:</strong> vordefinierte Rolle in einer KI, die Tonfall, Stil oder Fachperspektive bestimmt.</dd>

      <dt>Pitch</dt>
      <dd>Kurze, überzeugende Zusammenfassung einer Idee, meist mündlich präsentiert (eigentlich „Tonhöhe“).</dd>

      <dt>Prompt</dt>
      <dd>Eingabeaufforderung; Text oder Frage, die man einer Künstlichen Intelligenz (KI) gibt, um eine Antwort oder Aktion auszulösen.</dd>

      <dt>QR-Code</dt>
      <dd>Quick Response Code, lesbar für Smartphones, enthält z.B. Internetadressen.</dd>

      <dt>RAG</dt>
      <dd>Retrieval-Augmented Generation; KI bezieht zusätzlich externe Infos; ursprünglich für „tokenisierte“ Bibliotheken (heute auch als Baustein agentischer Systeme verwendet). Ein Service des Portals (z.B. Mistral, Le Chat) durchsucht zuvor aufbereitete Texte und wählt passende Abschnitte aus. Diese werden über das Kontextfenster an das eigentliche KI-Modell übergeben.</dd>

      <dt>Sampling-Parameter</dt>
      <dd>Steuern die Kreativität und Präzision von KI-Antworten. Sie legen fest, wie das Modell zwischen möglichen nächsten Wörtern wählt (z. B. über Temperature, Top-K, Top-P). Höhere Werte führen zu freieren, kreativ-variableren Antworten, niedrigere zu berechenbareren und konsistenteren Ergebnissen.</dd>

      <dt>Sokratischer Dialog</dt>
      <dd>Dialogische Methode, die mit klug gesetzten Denkanstößen dazu anregt, eigene Antworten und Erkenntnisse zu entwickeln.</dd>

      <dt>Speech-to-Text (S2T)</dt>
      <dd>Automatische Umwandlung von gesprochener Sprache in Text (z. B. für Diktierfunktionen). Das „2“ (engl. „two“) wird wie „to“ („zu“) ausgesprochen und ersetzt es im Kurzwort. <br>
        <span class="small">Weitere verwandte Technologien: <strong>Text-to-Speech (TTS)</strong>, <strong>Text-to-Video (T2V)</strong>.</span><br>
        <strong>Brain-to-Text (B2T):</strong> Forschungsfeld zur direkten Umwandlung von Gehirnaktivität in Text, mittels invasiver (Gehirn-Computer-Schnittstelle, BCI) oder nicht-invasiver Methoden (z. B. EEG, MEG). Stand 2025: Invasive Systeme erreichen bis zu 97,5 % Genauigkeit (Jayalath et al., 2025, arXiv:2505.13446) bei ausgewählten, intensiv trainierten Personen; nicht-invasive Verfahren bis zu 68 % (beste: 81 %).</dd>

      <dt>Thinking Mode</dt>
      <dd>Arbeitsmodus moderner KI-Systeme, bei dem die Modelle ausdrücklich zusätzliche Zeit zum „Nachdenken“ verwenden. Das Ergebnis sind oft ausführlichere, reflektierte Antworten mit größerem Kontextbezug oder kreativeren Lösungen. Diese Reflektionen sind häufig auch nachlesbar und gegebenenfalls zusammengefasst dargestellt.</dd>
    </dl>
  </section>

  <section id="ergaenzungen" class="section">
    <h2>Ergänzungen</h2>
    <h3 id="feinabstimmung">Verfahren zur Feinabstimmung und Anpassung von Sprachmodellen</h3>
    <dl>
      <dt>RLHF</dt>
      <dd>Reinforcement Learning from Human Feedback; Verstärkendes Lernen durch menschliche Bewertungen von Modellantworten; dient der Anpassung an gewünschte Werte, Stil und Sicherheit.</dd>

      <dt>RLAIF</dt>
      <dd>Reinforcement Learning from AI Feedback; Gleiche Methode, jedoch mit Feedback durch ein anderes KI-Modell statt durch Menschen – schneller und kostengünstiger.</dd>

      <dt>LoRA</dt>
      <dd>Low-Rank Adaptation; Methode zum Nachtraining großer Sprachmodelle, bei der kleine Zusatzgewichte („Adapter“) gelernt und beim Einsatz zum Basismodell addiert werden – das Original bleibt unverändert.</dd>

      <dt>QLoRA (Quantized LoRA)</dt>
      <dd>Speicher- und kostensparende Variante, die mit quantisierten (z. B. 4-Bit) Modellgewichten arbeitet und Nachtraining auf handelsüblichen GPUs ermöglicht.</dd>
    </dl>
  </section>

  <section id="training" class="section">
    <h3>Verfahren zum Training von Sprachmodellen</h3>
    <ol>
      <li>
        <strong>Pre-Training → Basismodell:</strong> Lernt auf sehr vielen Texten das nächste Wort vorherzusagen. <br>
        <span class="note">Sprachgefühl & Weltbezüge entstehen in den Gewichten.</span><br>
        <span class="small">(Pro: Transformer, Milliarden Parameter, Datenbereinigung & Deduplizierung).</span>
      </li>
      <li>
        <strong>Fine-Tuning & Alignment → Standard-Version:</strong> Mit Beispielen & Bewertungen wird das Verhalten <strong>hilfreich, ehrlich, harmlos</strong>. <br>
        <span class="small">System-Prompt/Regeln setzen Leitplanken.<br>
        (Pro: SFT, RLHF/DPO, Evals/Red-Teaming; Leitplanken = Laufzeit, keine Gewichtsänderung).</span>
      </li>
      <li>
        <strong>In-Context-Nutzung → Persona im Gespräch:</strong> Passt situativ Rolle, Stil, Aufgabe an – ohne Nachtrainieren.<br>
        <span class="small">(Pro: Kontextfenster, Few-Shot, optional RAG/Tools für Nachschlagen, Rechnen, Funktionen).</span>
      </li>
    </ol>
  </section>
</body>
</html>
